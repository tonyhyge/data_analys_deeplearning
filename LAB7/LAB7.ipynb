{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FBDzszTmi4E",
        "outputId": "1e2058ef-7bac-46e8-a517-10b3fb9d5417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gb = nltk.corpus.gutenberg\n",
        "print(\"Gutenberg files : \"\n",
        ", gb.fileids())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjQhSG8GmsMs",
        "outputId": "7bd05f6d-3958-4201-e724-b80caa085d2b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gutenberg files :  ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')"
      ],
      "metadata": {
        "id": "QFu2ZP9Sm4Eu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(macbeth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tqTqp_Pm51h",
        "outputId": "1fdc1d6a-1073-46fb-80c0-0316f2270cbb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23140"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth [:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQYZupi9m7k-",
        "outputId": "0be447dc-47f0-4c09-eb16-2358f02187d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'The',\n",
              " 'Tragedie',\n",
              " 'of',\n",
              " 'Macbeth',\n",
              " 'by',\n",
              " 'William',\n",
              " 'Shakespeare',\n",
              " '1603',\n",
              " ']']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e53vSZhtnCWx",
        "outputId": "5507c60a-1fc1-4524-bb6f-7c830914820c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth_sents = nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt')\n",
        "macbeth_sents[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuiJ3R1SnAI5",
        "outputId": "9f31af20-40a7-414d-a49d-376b40093a49"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['[',\n",
              "  'The',\n",
              "  'Tragedie',\n",
              "  'of',\n",
              "  'Macbeth',\n",
              "  'by',\n",
              "  'William',\n",
              "  'Shakespeare',\n",
              "  '1603',\n",
              "  ']'],\n",
              " ['Actus', 'Primus', '.'],\n",
              " ['Scoena', 'Prima', '.'],\n",
              " ['Thunder', 'and', 'Lightning', '.'],\n",
              " ['Enter', 'three', 'Witches', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = nltk.Text(macbeth)\n",
        "text.concordance('Stage')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Itu0CHCnFml",
        "outputId": "0867e914-fd24-4663-b0b8-e1d8b0e1159d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 3 of 3 matches:\n",
            "nts with Dishes and Seruice ouer the Stage . Then enter Macbeth Macb . If it we\n",
            "with mans Act , Threatens his bloody Stage : byth ' Clock ' tis Day , And yet d\n",
            " struts and frets his houre vpon the Stage , And then is heard no more . It is \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text.common_contexts(['Stage'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izm-E6hXnJim",
        "outputId": "972cfbfc-4ddf-4c15-9c8a-a96b772a10ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the_. bloody_: the_,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fd = nltk.FreqDist(macbeth)\n",
        "fd.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eflzRi2DnKqB",
        "outputId": "6d8c326f-0724-4076-94af-d74140d1f7d5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 1962),\n",
              " ('.', 1235),\n",
              " (\"'\", 637),\n",
              " ('the', 531),\n",
              " (':', 477),\n",
              " ('and', 376),\n",
              " ('I', 333),\n",
              " ('of', 315),\n",
              " ('to', 311),\n",
              " ('?', 241)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4hmB7RynMZM",
        "outputId": "4a30ad2f-0817-419d-adbf-e0fa019efc7f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sw = set(nltk.corpus.stopwords.words('english'))\n",
        "print(len(sw))\n",
        "list(sw)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGzqaVVQnNeP",
        "outputId": "7102eb21-565a-4618-f48d-6d084011f214"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "198\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['yours', 'once', 's', 'don', \"that'll\", 'with', 'off', 'my', 'nor', 'by']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth_filtered = [w for w in macbeth if w.lower() not in sw]\n",
        "len(macbeth_filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUy50q7cnO76",
        "outputId": "04e4a26c-48f1-4b62-f44c-9852ee61655e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14946"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fd = nltk.FreqDist(macbeth_filtered)\n",
        "fd.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH9k08LlnRRy",
        "outputId": "2d572097-2cdf-4894-e35b-77d1f8e433ec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 1962),\n",
              " ('.', 1235),\n",
              " (\"'\", 637),\n",
              " (':', 477),\n",
              " ('?', 241),\n",
              " ('Macb', 137),\n",
              " ('haue', 117),\n",
              " ('-', 100),\n",
              " ('Enter', 80),\n",
              " ('thou', 63)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "punctuation = set(string.punctuation)\n",
        "macbeth_filtered2 = [w.lower() for w in macbeth if w.lower() not in sw and w.lower() not in punctuation]\n",
        "fd = nltk.FreqDist(macbeth_filtered2)\n",
        "fd.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFZMQfGKnTWK",
        "outputId": "5e02efda-cbfd-4111-acf7-036588f3b0d0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('macb', 137),\n",
              " ('haue', 122),\n",
              " ('thou', 90),\n",
              " ('enter', 81),\n",
              " ('shall', 68),\n",
              " ('macbeth', 62),\n",
              " ('vpon', 62),\n",
              " ('thee', 61),\n",
              " ('macd', 58),\n",
              " ('vs', 57)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "long_words = [w for w in macbeth if len(w)> 12]\n",
        "sorted(long_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_oNVPqmnVIB",
        "outputId": "0b84e8ae-c518-4f98-c758-b99bb733c668"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Assassination',\n",
              " 'Chamberlaines',\n",
              " 'Distinguishes',\n",
              " 'Gallowgrosses',\n",
              " 'Metaphysicall',\n",
              " 'Northumberland',\n",
              " 'Voluptuousnesse',\n",
              " 'commendations',\n",
              " 'multitudinous',\n",
              " 'supernaturall',\n",
              " 'vnaccompanied']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ious_words = [w for w in macbeth if 'ious' in w]\n",
        "ious_words = set(ious_words)\n",
        "sorted(ious_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq6GP_hGnbBE",
        "outputId": "1075d126-82f3-47ec-cc12-b3fa3fb4d960"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Auaricious',\n",
              " 'Gracious',\n",
              " 'Industrious',\n",
              " 'Iudicious',\n",
              " 'Luxurious',\n",
              " 'Malicious',\n",
              " 'Obliuious',\n",
              " 'Pious',\n",
              " 'Rebellious',\n",
              " 'compunctious',\n",
              " 'furious',\n",
              " 'gracious',\n",
              " 'pernicious',\n",
              " 'pernitious',\n",
              " 'pious',\n",
              " 'precious',\n",
              " 'rebellious',\n",
              " 'sacrilegious',\n",
              " 'serious',\n",
              " 'spacious',\n",
              " 'tedious']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bgrms = nltk.FreqDist(nltk.bigrams(macbeth_filtered2))\n",
        "bgrms.most_common(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlHXGb6onfO2",
        "outputId": "73b5def5-3388-4c78-f2e7-0e75e4767e59"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('enter', 'macbeth'), 16),\n",
              " (('exeunt', 'scena'), 15),\n",
              " (('thane', 'cawdor'), 13),\n",
              " (('knock', 'knock'), 10),\n",
              " (('st', 'thou'), 9),\n",
              " (('thou', 'art'), 9),\n",
              " (('lord', 'macb'), 9),\n",
              " (('haue', 'done'), 8),\n",
              " (('macb', 'haue'), 8),\n",
              " (('good', 'lord'), 8),\n",
              " (('let', 'vs'), 7),\n",
              " (('enter', 'lady'), 7),\n",
              " (('wee', 'l'), 7),\n",
              " (('would', 'st'), 6),\n",
              " (('macbeth', 'macb'), 6)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tgrms = nltk.FreqDist(nltk.trigrams (macbeth_filtered2))\n",
        "tgrms.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JME6D9yWniX7",
        "outputId": "86188f4c-2829-4f88-8482-f90b0b51fc63"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('knock', 'knock', 'knock'), 6),\n",
              " (('enter', 'macbeth', 'macb'), 5),\n",
              " (('enter', 'three', 'witches'), 4),\n",
              " (('exeunt', 'scena', 'secunda'), 4),\n",
              " (('good', 'lord', 'macb'), 4),\n",
              " (('three', 'witches', '1'), 3),\n",
              " (('exeunt', 'scena', 'tertia'), 3),\n",
              " (('thunder', 'enter', 'three'), 3),\n",
              " (('exeunt', 'scena', 'quarta'), 3),\n",
              " (('scena', 'prima', 'enter'), 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "raw[:75]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "iV77lPP2nnp6",
        "outputId": "02c2b4e5-c750-4125-bf4f-c897f2f83a48"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "raw[:75]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3dz1fB17npX0",
        "outputId": "bc0193e5-30e5-458d-d23a-837666fba91e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize (raw)\n",
        "webtext = nltk.Text (tokens)\n",
        "webtext[:12]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y-Zw97lnryf",
        "outputId": "6d1facc9-dfa2-4220-9607-3e9d2d815ecf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['*',\n",
              " '*',\n",
              " '*',\n",
              " 'START',\n",
              " 'OF',\n",
              " 'THE',\n",
              " 'PROJECT',\n",
              " 'GUTENBERG',\n",
              " 'EBOOK',\n",
              " '2554',\n",
              " '*',\n",
              " '*']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
        "html = request.urlopen(url).read().decode('utf-8-sig')\n",
        "html[:120]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "S8tqomYwnxjq",
        "outputId": "67a784c7-71bc-4f82-9d2c-7c9b3363766e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">\\r\\n<html>\\r\\n<hea'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "raw = BeautifulSoup(html, \"lxml\").get_text()\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "text = nltk.Text(tokens)"
      ],
      "metadata": {
        "id": "RPp6AFp3n1T1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p7VJnoNn3Cs",
        "outputId": "2d41572b-1aec-4c1a-ba4f-96c8b104aa31"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "reviews = nltk.corpus.movie_reviews\n",
        "documents = [(list(reviews.words(fileid)), category)\n",
        "                for category in reviews.categories()\n",
        "                  for fileid in reviews.fileids(category)]\n",
        "random.shuffle(documents)"
      ],
      "metadata": {
        "id": "68KOkw_mn4UH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_review = ' '.join(documents[0][0])\n",
        "print(first_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdZYPrpnn5eV",
        "outputId": "f427b8c0-f293-4f4b-9d87-281e8d7cdfb1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linda fiorentino disappeared off the radar after a deservedly heralded turn in the cable pic the last seduction , and her being cast as dogma ' s lead is nothing short of inexplicable . she ' s still in fine form as bethany , an abortion clinic worker who ' s lost her faith . one night , a visitor from heaven makes a fiery entrance in bethany ' s bedroom . he is metatron ( alan rickman ) , the voice of god , and he needs her help : she must stop two fallen angels from entering a new jersey church - the fate of the universe depends on it . god would do it him / herself , but he / she is . . . missing , having taken up human form somewhere on earth never to be heard from again . bethany is joined on her road trip to the garden state by the \" prophets \" jay ( jason mewes ) and silent bob ( kevin smith , doing double - duty ) , the slacker minstrels who have appeared in all of smith ' s films thus far . at some point , rock drops naked out of the sky as rufus , the undocumented ( and very black ) \" thirteenth apostle \" , and offers his assistance , as does divine stripper serendipity ( salma hayek ) . it ' s a wild ride . they ' re in pursuit of loki and bartleby ( damon and affleck , respectively - this is probably the sharpest either has ever been ) , who were banished from heaven to wisconsin and have discovered a dogmatic loophole that will enable their return . loki decides to wreak havoc along the way with the knowledge that his sins will be absolved at the pearly gates . at one point , he terrorizes a boardroom full of suits with an angry combination of words and bullets . it ' s a nasty , guiltily enjoyable little scene that asks , \" how corrupt are you ? \" . wings of desire this ain ' t . since debuting with clerks , smith has grown as a director , particularly in terms of working with actors . ( chris rock is this film ' s only weak link - between jokes , he ' s wooden . ) his no - frills visual style hasn ' t changed much over the years , though ( dogma ' s widescreen compositions at least have blockbuster affectations ) , nor has his writing - his characters still sit around delivering one caustic , hilarious speech after another . dogma chips away at big religious issues - namely , the hypocrisy that accompanies any organized system of beliefs - eloquently and articulately , but a few of the monologues sound too much like blatant exposition . as well , the verbal introduction of each new person seems to take forever . any movie with this much weighty talk would have a hard time maintaining momentum ( hurlyburly , anyone ? ) , and eventually dogma ' s pacing goes slack . a long diatribe from bartleby late in the game , in which he laments the destiny of celestial beings , comes at a point when we ' ve heard enough . because his change of heart ( bartleby is initially the good cop to loki ' s bad ) drives the climax , said rant is given a great deal of screen time . sure , affleck deserved a big moment ( damon steals their scenes together prior ) , but it ultimately makes the film and us feel bloated . like tarantino , smith was a video - age sponge who became a sample - mad indie filmmaker . dogma pays welcome homage to an eclectic batch of movies , including indiana jones and the last crusade , with silent bob doing his best harrison ford , and weird science - a shit demon attacks our heroes ! smith also has a kitchen sink brand of humour : his dexterous maneuvering between the satirical ( a cardinal played by george carlin attempts to mount a publicity campaign with the slogan \" catholicism wow ! \" ) and the scatalogical ensures that no lover of comedy will leave dogma feeling malnourished . i bust ( ed ? ) a gut on several occasions . proceedings also get off on the right foot with the opening with the funniest disclaimer ever . it ' s a disclaimer unlikely to put protestors at ease , for to read it , one actually has to see dogma . the prerelease ballyhoo is in the tradition of the last temptation of christ ' s , martin scorsese ' s 1988 adaptation of nikos kazantzakis ' controversial novel , in that it is not directly linked to the picture ' s content but to rumours and heresy . there ' s a famous anecdote about fletch director michael ritchie inviting picketers of the last temptation of christ into a screening on his dime , just so they could know for certain what they were rallying against . ( not one of them had watched it . ) every single person refused . smith and scorsese have a lot in common , and so do the two films in question , because both smith ' s bethany and scorsese ' s jesus are hollow shells without their faith . in fact , dogma ' s denouement ( which follows a thrilling showdown that ' s worth the wait ) is a catholic love - in , a veritable recruitment poster . ( i felt sentimental about a religion i don ' t belong to . now that ' s powerful filmmaking . ) smith is nothing if not sincere about his own devotion to god , and that spirituality shines through . it ' s enough to make me forgive dogma for its editorial sins . my religion is movies . when the catholic league beats on dogma for imaginary crimes against a doctrine , in a roundabout way they ' re attacking what i live for : freedom of expression through celluloid . i therefore feel that , although i ' m no bible - thumper , i ' m at least as qualified to criticize dogma as william donohue and his followers .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sjzjuH6in-bv",
        "outputId": "7269a683-6166-4d89-f344-84d67bda339f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = nltk.FreqDist(w.lower() for w in reviews.words())\n",
        "word_features = list(all_words)"
      ],
      "metadata": {
        "id": "xrzIrzo5oLZs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def document_features(document, word_features):\n",
        "  document_words = set(document)\n",
        "  features = {}\n",
        "  for word in word_features:\n",
        "    features['{}'.format(word)] = (word in document_words)\n",
        "  return features"
      ],
      "metadata": {
        "id": "25gq_8z0oWwj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n",
        "len(featuresets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpjBr-W_oMkc",
        "outputId": "d04723a5-55a9-4f1f-bd35-6daec6aaeabf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = featuresets[1500:], featuresets[:500]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)"
      ],
      "metadata": {
        "id": "Ajbk2nCFoP5v"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_set, est_set = featuresets[1500:], featuresets[:500]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "print(nltk.classify.accuracy(classifier, test_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMMtX9CwoRnH",
        "outputId": "58fd0892-84c0-4199-e112-0588bf9a7b18"
      },
      "execution_count": 33,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.show_most_informative_features(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9yPpGQNoXnd",
        "outputId": "9954730b-5c41-47d3-95d6-76a31a031407"
      },
      "execution_count": 34,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "             wonderfully = True              pos : neg    =     10.9 : 1.0\n",
            "                  mature = True              pos : neg    =      9.4 : 1.0\n",
            "                strength = True              pos : neg    =      9.4 : 1.0\n",
            "                visually = True              pos : neg    =      9.4 : 1.0\n",
            "                everyday = True              pos : neg    =      8.6 : 1.0\n",
            "                portrays = True              pos : neg    =      8.6 : 1.0\n",
            "                    bore = True              neg : pos    =      8.0 : 1.0\n",
            "                 cartoon = True              neg : pos    =      8.0 : 1.0\n",
            "                   views = True              pos : neg    =      7.9 : 1.0\n",
            "               portrayal = True              pos : neg    =      7.2 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "câu 1\n"
      ],
      "metadata": {
        "id": "mixb3dbaP2PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_names = nltk.corpus.gutenberg.fileids()\n",
        "print(\"Danh sách các tên corpus:\")\n",
        "for name in corpus_names:\n",
        "    print(name)"
      ],
      "metadata": {
        "id": "0ahlZNeVpA8h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f47205d-2d21-44c3-dd9f-5ec33419639f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Danh sách các tên corpus:\n",
            "austen-emma.txt\n",
            "austen-persuasion.txt\n",
            "austen-sense.txt\n",
            "bible-kjv.txt\n",
            "blake-poems.txt\n",
            "bryant-stories.txt\n",
            "burgess-busterbrown.txt\n",
            "carroll-alice.txt\n",
            "chesterton-ball.txt\n",
            "chesterton-brown.txt\n",
            "chesterton-thursday.txt\n",
            "edgeworth-parents.txt\n",
            "melville-moby_dick.txt\n",
            "milton-paradise.txt\n",
            "shakespeare-caesar.txt\n",
            "shakespeare-hamlet.txt\n",
            "shakespeare-macbeth.txt\n",
            "whitman-leaves.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "languages = nltk.corpus.stopwords.fileids()\n",
        "\n",
        "for language in languages:\n",
        "    print(f\"Ngôn ngữ: {language}\")\n",
        "    stopwords = nltk.corpus.stopwords.words(language)\n",
        "    print(f\"Số lượng stopwords: {len(stopwords)}\")\n",
        "    print(f\"10 stopwords đầu tiên: {stopwords[:10]}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPkBprx5P3gX",
        "outputId": "289cae5f-f9e4-47fd-f50d-783b0b12c5f8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngôn ngữ: albanian\n",
            "Số lượng stopwords: 237\n",
            "10 stopwords đầu tiên: ['tyre', 'rreth', 'le', 'atyre', 'këta', 'megjithëse', 'kemi', 'per', 'ndonëse', 'dytë']\n",
            "--------------------\n",
            "Ngôn ngữ: arabic\n",
            "Số lượng stopwords: 754\n",
            "10 stopwords đầu tiên: ['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي']\n",
            "--------------------\n",
            "Ngôn ngữ: azerbaijani\n",
            "Số lượng stopwords: 165\n",
            "10 stopwords đầu tiên: ['a', 'ad', 'altı', 'altmış', 'amma', 'arasında', 'artıq', 'ay', 'az', 'bax']\n",
            "--------------------\n",
            "Ngôn ngữ: basque\n",
            "Số lượng stopwords: 326\n",
            "10 stopwords đầu tiên: ['ahala', 'aitzitik', 'al', 'ala ', 'alabadere', 'alabaina', 'alabaina', 'aldiz ', 'alta', 'amaitu']\n",
            "--------------------\n",
            "Ngôn ngữ: belarusian\n",
            "Số lượng stopwords: 224\n",
            "10 stopwords đầu tiên: ['на', 'не', 'што', 'па', 'да', 'за', 'як', 'для', 'гэта', 'ад']\n",
            "--------------------\n",
            "Ngôn ngữ: bengali\n",
            "Số lượng stopwords: 398\n",
            "10 stopwords đầu tiên: ['অতএব', 'অথচ', 'অথবা', 'অনুযায়ী', 'অনেক', 'অনেকে', 'অনেকেই', 'অন্তত', 'অন্য', 'অবধি']\n",
            "--------------------\n",
            "Ngôn ngữ: catalan\n",
            "Số lượng stopwords: 278\n",
            "10 stopwords đầu tiên: ['a', 'abans', 'ací', 'ah', 'així', 'això', 'al', 'aleshores', 'algun', 'alguna']\n",
            "--------------------\n",
            "Ngôn ngữ: chinese\n",
            "Số lượng stopwords: 841\n",
            "10 stopwords đầu tiên: ['一', '一下', '一些', '一切', '一则', '一天', '一定', '一方面', '一旦', '一时']\n",
            "--------------------\n",
            "Ngôn ngữ: danish\n",
            "Số lượng stopwords: 94\n",
            "10 stopwords đầu tiên: ['og', 'i', 'jeg', 'det', 'at', 'en', 'den', 'til', 'er', 'som']\n",
            "--------------------\n",
            "Ngôn ngữ: dutch\n",
            "Số lượng stopwords: 101\n",
            "10 stopwords đầu tiên: ['de', 'en', 'van', 'ik', 'te', 'dat', 'die', 'in', 'een', 'hij']\n",
            "--------------------\n",
            "Ngôn ngữ: english\n",
            "Số lượng stopwords: 198\n",
            "10 stopwords đầu tiên: ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n",
            "--------------------\n",
            "Ngôn ngữ: finnish\n",
            "Số lượng stopwords: 235\n",
            "10 stopwords đầu tiên: ['olla', 'olen', 'olet', 'on', 'olemme', 'olette', 'ovat', 'ole', 'oli', 'olisi']\n",
            "--------------------\n",
            "Ngôn ngữ: french\n",
            "Số lượng stopwords: 157\n",
            "10 stopwords đầu tiên: ['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']\n",
            "--------------------\n",
            "Ngôn ngữ: german\n",
            "Số lượng stopwords: 232\n",
            "10 stopwords đầu tiên: ['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an']\n",
            "--------------------\n",
            "Ngôn ngữ: greek\n",
            "Số lượng stopwords: 265\n",
            "10 stopwords đầu tiên: ['αλλα', 'αν', 'αντι', 'απο', 'αυτα', 'αυτεσ', 'αυτη', 'αυτο', 'αυτοι', 'αυτοσ']\n",
            "--------------------\n",
            "Ngôn ngữ: hebrew\n",
            "Số lượng stopwords: 221\n",
            "10 stopwords đầu tiên: ['אני', 'את', 'אתה', 'אנחנו', 'אתן', 'אתם', 'הם', 'הן', 'היא', 'הוא']\n",
            "--------------------\n",
            "Ngôn ngữ: hinglish\n",
            "Số lượng stopwords: 1036\n",
            "10 stopwords đầu tiên: ['a', 'aadi', 'aaj', 'aap', 'aapne', 'aata', 'aati', 'aaya', 'aaye', 'ab']\n",
            "--------------------\n",
            "Ngôn ngữ: hungarian\n",
            "Số lượng stopwords: 199\n",
            "10 stopwords đầu tiên: ['a', 'ahogy', 'ahol', 'aki', 'akik', 'akkor', 'alatt', 'által', 'általában', 'amely']\n",
            "--------------------\n",
            "Ngôn ngữ: indonesian\n",
            "Số lượng stopwords: 758\n",
            "10 stopwords đầu tiên: ['ada', 'adalah', 'adanya', 'adapun', 'agak', 'agaknya', 'agar', 'akan', 'akankah', 'akhir']\n",
            "--------------------\n",
            "Ngôn ngữ: italian\n",
            "Số lượng stopwords: 279\n",
            "10 stopwords đầu tiên: ['ad', 'al', 'allo', 'ai', 'agli', 'all', 'agl', 'alla', 'alle', 'con']\n",
            "--------------------\n",
            "Ngôn ngữ: kazakh\n",
            "Số lượng stopwords: 324\n",
            "10 stopwords đầu tiên: ['ах', 'ох', 'эх', 'ай', 'эй', 'ой', 'тағы', 'тағыда', 'әрине', 'жоқ']\n",
            "--------------------\n",
            "Ngôn ngữ: nepali\n",
            "Số lượng stopwords: 255\n",
            "10 stopwords đầu tiên: ['छ', 'र', 'पनि', 'छन्', 'लागि', 'भएको', 'गरेको', 'भने', 'गर्न', 'गर्ने']\n",
            "--------------------\n",
            "Ngôn ngữ: norwegian\n",
            "Số lượng stopwords: 176\n",
            "10 stopwords đầu tiên: ['og', 'i', 'jeg', 'det', 'at', 'en', 'et', 'den', 'til', 'er']\n",
            "--------------------\n",
            "Ngôn ngữ: portuguese\n",
            "Số lượng stopwords: 207\n",
            "10 stopwords đầu tiên: ['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as']\n",
            "--------------------\n",
            "Ngôn ngữ: romanian\n",
            "Số lượng stopwords: 356\n",
            "10 stopwords đầu tiên: ['a', 'abia', 'acea', 'aceasta', 'această', 'aceea', 'aceeasi', 'acei', 'aceia', 'acel']\n",
            "--------------------\n",
            "Ngôn ngữ: russian\n",
            "Số lượng stopwords: 151\n",
            "10 stopwords đầu tiên: ['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со']\n",
            "--------------------\n",
            "Ngôn ngữ: slovene\n",
            "Số lượng stopwords: 1784\n",
            "10 stopwords đầu tiên: ['ali', 'ampak', 'bodisi', 'in', 'kajti', 'marveč', 'namreč', 'ne', 'niti', 'oziroma']\n",
            "--------------------\n",
            "Ngôn ngữ: spanish\n",
            "Số lượng stopwords: 313\n",
            "10 stopwords đầu tiên: ['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']\n",
            "--------------------\n",
            "Ngôn ngữ: swedish\n",
            "Số lượng stopwords: 114\n",
            "10 stopwords đầu tiên: ['och', 'det', 'att', 'i', 'en', 'jag', 'hon', 'som', 'han', 'på']\n",
            "--------------------\n",
            "Ngôn ngữ: tajik\n",
            "Số lượng stopwords: 163\n",
            "10 stopwords đầu tiên: ['аз', 'дар', 'ба', 'бо', 'барои', 'бе', 'то', 'ҷуз', 'пеши', 'назди']\n",
            "--------------------\n",
            "Ngôn ngữ: tamil\n",
            "Số lượng stopwords: 125\n",
            "10 stopwords đầu tiên: ['அங்கு', 'அங்கே', 'அடுத்த', 'அதனால்', 'அதன்', 'அதற்கு', 'அதிக', 'அதில்', 'அது', 'அதே']\n",
            "--------------------\n",
            "Ngôn ngữ: turkish\n",
            "Số lượng stopwords: 53\n",
            "10 stopwords đầu tiên: ['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz']\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "câu 3"
      ],
      "metadata": {
        "id": "YHSxAoL2SB7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "languages = nltk.corpus.stopwords.fileids()\n",
        "\n",
        "for language in languages:\n",
        "    print(f\"Ngôn ngữ: {language}\")\n",
        "    stopwords = nltk.corpus.stopwords.words(language)\n",
        "    print(f\"Số lượng stopwords: {len(stopwords)}\")\n",
        "    print(f\"5 stopwords đầu tiên: {stopwords[:5]}\")\n",
        "    print(f\"5 stopwords cuối cùng: {stopwords[-5:]}\") # Kiểm tra 5 stopword cuối\n",
        "    print(\" \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pvrO2b9ROHn",
        "outputId": "8e507360-bf48-449b-9b60-4c0ccfc5f8ef"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngôn ngữ: albanian\n",
            "Số lượng stopwords: 237\n",
            "5 stopwords đầu tiên: ['tyre', 'rreth', 'le', 'atyre', 'këta']\n",
            "5 stopwords cuối cùng: ['ndonese', 'tani', 'pak', 'e', 'shumë']\n",
            " \n",
            "Ngôn ngữ: arabic\n",
            "Số lượng stopwords: 754\n",
            "5 stopwords đầu tiên: ['إذ', 'إذا', 'إذما', 'إذن', 'أف']\n",
            "5 stopwords cuối cùng: ['علق', 'قام', 'كرب', 'كاد', 'هبّ']\n",
            " \n",
            "Ngôn ngữ: azerbaijani\n",
            "Số lượng stopwords: 165\n",
            "5 stopwords đầu tiên: ['a', 'ad', 'altı', 'altmış', 'amma']\n",
            "5 stopwords cuối cùng: ['yox', 'yoxdur', 'yoxsa', 'yüz', 'zaman']\n",
            " \n",
            "Ngôn ngữ: basque\n",
            "Số lượng stopwords: 326\n",
            "5 stopwords đầu tiên: ['ahala', 'aitzitik', 'al', 'ala ', 'alabadere']\n",
            "5 stopwords cuối cùng: ['zu', 'zuek', 'zuen', 'zuten', 'zuzen']\n",
            " \n",
            "Ngôn ngữ: belarusian\n",
            "Số lượng stopwords: 224\n",
            "5 stopwords đầu tiên: ['на', 'не', 'што', 'па', 'да']\n",
            "5 stopwords cuối cùng: ['другi', 'удзень', 'нiчога', 'мой', 'побач']\n",
            " \n",
            "Ngôn ngữ: bengali\n",
            "Số lượng stopwords: 398\n",
            "5 stopwords đầu tiên: ['অতএব', 'অথচ', 'অথবা', 'অনুযায়ী', 'অনেক']\n",
            "5 stopwords cuối cùng: ['হাজার', 'হিসাবে', 'হৈলে', 'হোক', 'হয়']\n",
            " \n",
            "Ngôn ngữ: catalan\n",
            "Số lượng stopwords: 278\n",
            "5 stopwords đầu tiên: ['a', 'abans', 'ací', 'ah', 'així']\n",
            "5 stopwords cuối cùng: ['éreu', 'és', 'éssent', 'últim', 'ús']\n",
            " \n",
            "Ngôn ngữ: chinese\n",
            "Số lượng stopwords: 841\n",
            "5 stopwords đầu tiên: ['一', '一下', '一些', '一切', '一则']\n",
            "5 stopwords cuối cùng: ['顺', '顺着', '首先', '高兴', '是不是']\n",
            " \n",
            "Ngôn ngữ: danish\n",
            "Số lượng stopwords: 94\n",
            "5 stopwords đầu tiên: ['og', 'i', 'jeg', 'det', 'at']\n",
            "5 stopwords cuối cùng: ['hendes', 'været', 'thi', 'jer', 'sådan']\n",
            " \n",
            "Ngôn ngữ: dutch\n",
            "Số lượng stopwords: 101\n",
            "5 stopwords đầu tiên: ['de', 'en', 'van', 'ik', 'te']\n",
            "5 stopwords cuối cùng: ['niets', 'uw', 'iemand', 'geweest', 'andere']\n",
            " \n",
            "Ngôn ngữ: english\n",
            "Số lượng stopwords: 198\n",
            "5 stopwords đầu tiên: ['a', 'about', 'above', 'after', 'again']\n",
            "5 stopwords cuối cùng: [\"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
            " \n",
            "Ngôn ngữ: finnish\n",
            "Số lượng stopwords: 235\n",
            "5 stopwords đầu tiên: ['olla', 'olen', 'olet', 'on', 'olemme']\n",
            "5 stopwords cuối cùng: ['yli', 'kun', 'niin', 'nyt', 'itse']\n",
            " \n",
            "Ngôn ngữ: french\n",
            "Số lượng stopwords: 157\n",
            "5 stopwords đầu tiên: ['au', 'aux', 'avec', 'ce', 'ces']\n",
            "5 stopwords cuối cùng: ['eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n",
            " \n",
            "Ngôn ngữ: german\n",
            "Số lượng stopwords: 232\n",
            "5 stopwords đầu tiên: ['aber', 'alle', 'allem', 'allen', 'aller']\n",
            "5 stopwords cuối cùng: ['zu', 'zum', 'zur', 'zwar', 'zwischen']\n",
            " \n",
            "Ngôn ngữ: greek\n",
            "Số lượng stopwords: 265\n",
            "5 stopwords đầu tiên: ['αλλα', 'αν', 'αντι', 'απο', 'αυτα']\n",
            "5 stopwords cuối cùng: ['ὡσ', 'ὥς', 'ὥστε', 'ὦ', 'ᾧ']\n",
            " \n",
            "Ngôn ngữ: hebrew\n",
            "Số lượng stopwords: 221\n",
            "5 stopwords đầu tiên: ['אני', 'את', 'אתה', 'אנחנו', 'אתן']\n",
            "5 stopwords cuối cùng: ['אחרת', 'אחרים', 'אחרות', 'אשר', 'או']\n",
            " \n",
            "Ngôn ngữ: hinglish\n",
            "Số lượng stopwords: 1036\n",
            "5 stopwords đầu tiên: ['a', 'aadi', 'aaj', 'aap', 'aapne']\n",
            "5 stopwords cuối cùng: ['yours', 'yourself', 'yourselves', \"you've\", 'yup']\n",
            " \n",
            "Ngôn ngữ: hungarian\n",
            "Số lượng stopwords: 199\n",
            "5 stopwords đầu tiên: ['a', 'ahogy', 'ahol', 'aki', 'akik']\n",
            "5 stopwords cuối cùng: ['voltunk', 'vissza', 'vele', 'viszont', 'volna']\n",
            " \n",
            "Ngôn ngữ: indonesian\n",
            "Số lượng stopwords: 758\n",
            "5 stopwords đầu tiên: ['ada', 'adalah', 'adanya', 'adapun', 'agak']\n",
            "5 stopwords cuối cùng: ['wong', 'yaitu', 'yakin', 'yakni', 'yang']\n",
            " \n",
            "Ngôn ngữ: italian\n",
            "Số lượng stopwords: 279\n",
            "5 stopwords đầu tiên: ['ad', 'al', 'allo', 'ai', 'agli']\n",
            "5 stopwords cuối cùng: ['stessi', 'stesse', 'stessimo', 'stessero', 'stando']\n",
            " \n",
            "Ngôn ngữ: kazakh\n",
            "Số lượng stopwords: 324\n",
            "5 stopwords đầu tiên: ['ах', 'ох', 'эх', 'ай', 'эй']\n",
            "5 stopwords cuối cùng: ['осындай', 'ғана', 'қана', 'тек', 'әншейін']\n",
            " \n",
            "Ngôn ngữ: nepali\n",
            "Số lượng stopwords: 255\n",
            "5 stopwords đầu tiên: ['छ', 'र', 'पनि', 'छन्', 'लागि']\n",
            "5 stopwords cuối cùng: ['सारा', 'सोही', 'स्पष्ट', 'हरे', 'हरेक']\n",
            " \n",
            "Ngôn ngữ: norwegian\n",
            "Số lượng stopwords: 176\n",
            "5 stopwords đầu tiên: ['og', 'i', 'jeg', 'det', 'at']\n",
            "5 stopwords cuối cùng: ['vore', 'verte', 'vort', 'varte', 'vart']\n",
            " \n",
            "Ngôn ngữ: portuguese\n",
            "Số lượng stopwords: 207\n",
            "5 stopwords đầu tiên: ['a', 'à', 'ao', 'aos', 'aquela']\n",
            "5 stopwords cuối cùng: ['um', 'uma', 'você', 'vocês', 'vos']\n",
            " \n",
            "Ngôn ngữ: romanian\n",
            "Số lượng stopwords: 356\n",
            "5 stopwords đầu tiên: ['a', 'abia', 'acea', 'aceasta', 'această']\n",
            "5 stopwords cuối cùng: ['ăstea', 'ăştia', 'şi', 'ţi', 'ţie']\n",
            " \n",
            "Ngôn ngữ: russian\n",
            "Số lượng stopwords: 151\n",
            "5 stopwords đầu tiên: ['и', 'в', 'во', 'не', 'что']\n",
            "5 stopwords cuối cùng: ['более', 'всегда', 'конечно', 'всю', 'между']\n",
            " \n",
            "Ngôn ngữ: slovene\n",
            "Số lượng stopwords: 1784\n",
            "5 stopwords đầu tiên: ['ali', 'ampak', 'bodisi', 'in', 'kajti']\n",
            "5 stopwords cuối cùng: ['maralo', 'moglo', 'moralo', 'smelo', 'zmogl']\n",
            " \n",
            "Ngôn ngữ: spanish\n",
            "Số lượng stopwords: 313\n",
            "5 stopwords đầu tiên: ['de', 'la', 'que', 'el', 'en']\n",
            "5 stopwords cuối cùng: ['tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n",
            " \n",
            "Ngôn ngữ: swedish\n",
            "Số lượng stopwords: 114\n",
            "5 stopwords đầu tiên: ['och', 'det', 'att', 'i', 'en']\n",
            "5 stopwords cuối cùng: ['vårt', 'våra', 'ert', 'era', 'vilkas']\n",
            " \n",
            "Ngôn ngữ: tajik\n",
            "Số lượng stopwords: 163\n",
            "5 stopwords đầu tiên: ['аз', 'дар', 'ба', 'бо', 'барои']\n",
            "5 stopwords cuối cùng: ['шояд ки', 'охир', 'аз рӯи', 'аз рӯйи ', 'рӯ']\n",
            " \n",
            "Ngôn ngữ: tamil\n",
            "Số lượng stopwords: 125\n",
            "5 stopwords đầu tiên: ['அங்கு', 'அங்கே', 'அடுத்த', 'அதனால்', 'அதன்']\n",
            "5 stopwords cuối cùng: ['வரையில்', 'விட', 'விட்டு', 'வேண்டும்', 'வேறு']\n",
            " \n",
            "Ngôn ngữ: turkish\n",
            "Số lượng stopwords: 53\n",
            "5 stopwords đầu tiên: ['acaba', 'ama', 'aslında', 'az', 'bazı']\n",
            "5 stopwords cuối cùng: ['tüm', 've', 'veya', 'ya', 'yani']\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "câu 4"
      ],
      "metadata": {
        "id": "p6L6G2TJSMdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is an example text with some stop words to be removed.\"\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "filtered_text = \" \".join(filtered_words)\n",
        "\n",
        "print(filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tEQm3EHSC91",
        "outputId": "4a4a0dd1-09cb-4ed5-b902-700ceddc9c4b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example text stop words removed .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "câu 5"
      ],
      "metadata": {
        "id": "pTwD9gK8UIi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Danh sách stopwords cần bỏ qua\n",
        "ignored_stopwords = {\"not\", \"no\"} # Ví dụ, bỏ qua \"not\" và \"no\"\n",
        "\n",
        "# Loại bỏ ignored_stopwords khỏi stop_words\n",
        "stop_words = stop_words - ignored_stopwords\n",
        "\n",
        "# Văn bản đầu vào\n",
        "text = \"This is an example text with some stop words, not to be removed.\"\n",
        "\n",
        "# Tách văn bản thành các từ\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Loại bỏ stopwords, ngoại trừ ignored_stopwords\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Nối các từ đã lọc lại thành văn bản\n",
        "filtered_text = \" \".join(filtered_words)\n",
        "\n",
        "# In văn bản đã lọc\n",
        "print(filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWazdXAoTEen",
        "outputId": "cae2da35-07e4-4bd3-8a4c-3a29256c3fb9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example text stop words , not removed .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "câu 6\n"
      ],
      "metadata": {
        "id": "WKzShmKgUKCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia\n",
        "word = \"car\"\n",
        "import wikipedia\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "synsets = wordnet.synsets(word)\n",
        "if synsets:\n",
        "    definition = synsets[0].definition()\n",
        "    examples = synsets[0].examples()\n",
        "    print(\"WordNet:\")\n",
        "    print(\"Definition:\", definition)\n",
        "    print(\"Examples:\", examples)\n",
        "\n",
        "# Wikipedia\n",
        "try:\n",
        "    page = wikipedia.page(word)\n",
        "    summary = page.summary\n",
        "    print(\"\\nWikipedia:\")\n",
        "    print(\"Summary:\", summary[:200]) # In 200 ký tự đầu tiên\n",
        "except wikipedia.exceptions.PageError:\n",
        "    print(\"Không tìm thấy trang Wikipedia cho từ này.\")\n",
        "except wikipedia.exceptions.DisambiguationError as e:\n",
        "    print(\"Nhiều trang Wikipedia khớp với từ này:\", e.options)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFCcxjsHTk9h",
        "outputId": "f21bf49a-956b-4616-d1d4-98d6c56c7507"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.12.2)\n",
            "WordNet:\n",
            "Definition: a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
            "Examples: ['he needs a car to get to work']\n",
            "\n",
            "Wikipedia:\n",
            "Summary: The cat (Felis catus), also referred to as the domestic cat or house cat, is a small domesticated carnivorous mammal. It is the only domesticated species of the family Felidae. Advances in archaeology\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "câu 7"
      ],
      "metadata": {
        "id": "PRYkgzGEWoU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "word = \"good\"\n",
        "\n",
        "synonyms = []\n",
        "antonyms = []\n",
        "\n",
        "for syn in wn.synsets(word):\n",
        "    for l in syn.lemmas():\n",
        "        synonyms.append(l.name())\n",
        "        if l.antonyms():\n",
        "            antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "print(\"Synonyms:\", set(synonyms))\n",
        "print(\"Antonyms:\", set(antonyms))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVG1lOqtUM36",
        "outputId": "30edac39-8f22-4850-b11f-0f4c3e1a392a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonyms: {'skillful', 'good', 'salutary', 'undecomposed', 'well', 'proficient', 'expert', 'sound', 'adept', 'unspoiled', 'secure', 'skilful', 'unspoilt', 'upright', 'honorable', 'effective', 'honest', 'respectable', 'dear', 'in_force', 'soundly', 'safe', 'goodness', 'serious', 'just', 'near', 'beneficial', 'in_effect', 'trade_good', 'dependable', 'commodity', 'ripe', 'estimable', 'practiced', 'full', 'right', 'thoroughly'}\n",
            "Antonyms: {'evilness', 'evil', 'bad', 'ill', 'badness'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "câu 8"
      ],
      "metadata": {
        "id": "6mcQGwK0Wyjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('tagsets')\n",
        "nltk.download('tagsets_json')\n",
        "\n",
        "print(nltk.help.upenn_tagset())\n",
        "print(nltk.help.upenn_tagset('NN'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPVV3-uoWrcc",
        "outputId": "5f69db36-69a7-4d4c-fdef-33741032c1d9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n",
            "None\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package tagsets_json to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets_json is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "câu 9 10"
      ],
      "metadata": {
        "id": "ESBwleiXX4P1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def similarity(word1, word2, pos=None):\n",
        "    \"\"\"Tính toán độ giống nhau giữa hai từ.\"\"\"\n",
        "    synsets1 = wn.synsets(word1, pos=pos)\n",
        "    synsets2 = wn.synsets(word2, pos=pos)\n",
        "    if not synsets1 or not synsets2:\n",
        "        return 0\n",
        "\n",
        "    max_similarity = 0\n",
        "    for syn1 in synsets1:\n",
        "        for syn2 in synsets2:\n",
        "            similarity = syn1.wup_similarity(syn2) # Sử dụng Wu-Palmer similarity\n",
        "            if similarity is not None and similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "    return max_similarity\n",
        "\n",
        "# So sánh danh từ\n",
        "noun1 = \"car\"\n",
        "noun2 = \"automobile\"\n",
        "noun_similarity = similarity(noun1, noun2, pos=wn.NOUN)\n",
        "print(\"Noun similarity:\", noun_similarity)\n",
        "\n",
        "# So sánh động từ\n",
        "verb1 = \"eat\"\n",
        "verb2 = \"consume\"\n",
        "verb_similarity = similarity(verb1, verb2, pos=wn.VERB)\n",
        "print(\"Verb similarity:\", verb_similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPDni5r1W-HY",
        "outputId": "783a2483-727f-4a0d-a707-9743cd812c04"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun similarity: 1.0\n",
            "Verb similarity: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11 12 13"
      ],
      "metadata": {
        "id": "AcBVvJgtZoR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.corpus import names\n",
        "\n",
        "nltk.download('names')\n",
        "\n",
        "# 11. Số lượng và 10 tên đầu tiên\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')\n",
        "\n",
        "print(\"Số lượng tên nam:\", len(male_names))\n",
        "print(\"10 tên nam đầu tiên:\", male_names[:10])\n",
        "\n",
        "print(\"\\nSố lượng tên nữ:\", len(female_names))\n",
        "print(\"10 tên nữ đầu tiên:\", female_names[:10])\n",
        "\n",
        "\n",
        "# 12. 15 kết hợp ngẫu nhiên\n",
        "import random\n",
        "\n",
        "labeled_names = ([(name, 'male') for name in male_names] +\n",
        "                 [(name, 'female') for name in female_names])\n",
        "random.shuffle(labeled_names)\n",
        "print(\"\\n15 kết hợp ngẫu nhiên:\")\n",
        "for name, label in labeled_names[:15]:\n",
        "    print(name, label)\n",
        "\n",
        "\n",
        "# 13. Trích xuất ký tự cuối cùng và tạo mảng mới\n",
        "last_letters = [(name[-1], label) for name, label in labeled_names]\n",
        "print(\"\\nMảng mới với ký tự cuối cùng và nhãn:\")\n",
        "print(last_letters[:10]) # In 10 phần tử đầu tiên"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rBEtCxpX5nJ",
        "outputId": "b1708d80-e41f-4872-cc5f-8a8663c4add0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số lượng tên nam: 2943\n",
            "10 tên nam đầu tiên: ['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot', 'Abbott', 'Abby', 'Abdel', 'Abdul', 'Abdulkarim']\n",
            "\n",
            "Số lượng tên nữ: 5001\n",
            "10 tên nữ đầu tiên: ['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', 'Abby', 'Abigael', 'Abigail', 'Abigale']\n",
            "\n",
            "15 kết hợp ngẫu nhiên:\n",
            "Lowell male\n",
            "Blanca female\n",
            "Ebony female\n",
            "Dusty female\n",
            "Zorro male\n",
            "Idette female\n",
            "Abdul male\n",
            "Nelsen male\n",
            "Joshuah male\n",
            "Teressa female\n",
            "Daphene female\n",
            "Elizabet female\n",
            "Shaylah female\n",
            "Marlon male\n",
            "Jerrilyn female\n",
            "\n",
            "Mảng mới với ký tự cuối cùng và nhãn:\n",
            "[('l', 'male'), ('a', 'female'), ('y', 'female'), ('y', 'female'), ('o', 'male'), ('e', 'female'), ('l', 'male'), ('n', 'male'), ('h', 'male'), ('a', 'female')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YmUeGXHbZqPG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}